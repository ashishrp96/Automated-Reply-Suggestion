<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Automated Reply Suggestion Using Natural Language Processing Techniques</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Automated Reply Suggestion Using Natural Language Processing Techniques</h1>
</header>
<section data-field="subtitle" class="p-summary">
All the smartphones are equipped with smart keyboards that suggest words and replies to your chat messages. Gmail by Google provides a…
</section>
<section data-field="body" class="e-content">
<section name="4060" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="bbca" id="bbca" class="graf graf--h3 graf--leading graf--title">Automated Reply Suggestion Using Natural Language Processing Techniques</h3></div><div class="section-inner sectionLayout--outsetColumn"><figure name="f876" id="f876" class="graf graf--figure graf--layoutOutsetCenter graf-after--h3"><img class="graf-image" data-image-id="1*gQgciY2E6OZT-eLsh1J_4A.png" data-width="1200" data-height="800" data-is-featured="true" src="https://cdn-images-1.medium.com/max/1200/1*gQgciY2E6OZT-eLsh1J_4A.png"><figcaption class="imageCaption">Gmail’s Smart Reply</figcaption></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="35a9" id="35a9" class="graf graf--p graf-after--figure">All the smartphones are equipped with smart keyboards that suggest words and replies to your chat messages. Gmail by Google provides a smart reply suggestion feature for incoming mails. You’ll see three reply options based on the contents of the incoming email. We need a fast, simple deep learning architecture to suggest replies for incoming messages for your chat apps. If you get an incoming message, for example, “How are you doing today?”, you need smart reply suggestions like “I’m doing good”, “Great!”, “Good and how are you?” etc., for quick conversations that will improve user experience by leaps and bounds on chat apps. We built the reply suggestion system for Slingshot App, an enterprise task management application with a chat feature where co-workers can chat. Our goal was to create a smart reply suggestion model that would suggest replies which could work regardless of the context or specifics of the conversation that would enhance the user experience but doesn’t annoy the user.</p><p name="b168" id="b168" class="graf graf--p graf-after--p">Since there are some models and papers on this topic, we chose <a href="https://www.kdd.org/kdd2016/papers/files/Paper_1069.pdf" data-href="https://www.kdd.org/kdd2016/papers/files/Paper_1069.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Google’s Smart Reply Paper</a> as our base for our model architecture and also got some useful methods from this <a href="https://towardsdatascience.com/training-your-own-message-suggestions-model-using-deep-learning-3609c0057ba8" data-href="https://towardsdatascience.com/training-your-own-message-suggestions-model-using-deep-learning-3609c0057ba8" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Towards Data Science Article by Rakshit Pujari</a>. We explored and tried various variations on the existing model.</p><h3 name="57c6" id="57c6" class="graf graf--h3 graf-after--p">Data Collection and Cleaning</h3><p name="b8c6" id="b8c6" class="graf graf--p graf-after--h3">Our use case was specific for business or professional communication between co-workers. we had to gather a dataset with both general questions and answers, professional conversations. So we chose three datasets from a wide range of chat data corpi along with one of our own:</p><ol class="postList"><li name="e5c0" id="e5c0" class="graf graf--li graf-after--p"><a href="https://www.kaggle.com/thoughtvector/customer-support-on-twitter?ref=hackernoon.com" data-href="https://www.kaggle.com/thoughtvector/customer-support-on-twitter?ref=hackernoon.com" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Twitter chat dataset</a> — This data is the most helpful data in terms of professional conversations as it is a dataset with conversations between brand support accounts with customers. It may consist of some inappropriate messages but they can be filtered and cleaned out. It has more than two million records.</li><li name="6eb9" id="6eb9" class="graf graf--li graf-after--li"><a href="https://www.kaggle.com/arnavsharmaas/chatbot-dataset-topical-chat" data-href="https://www.kaggle.com/arnavsharmaas/chatbot-dataset-topical-chat" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon chat dataset</a> — This data is useful for the generalization of the dataset as it contains general knowledge-based conversations. It also had many relevant nuggets of conversations that can steer the model in the right direction.</li><li name="f941" id="f941" class="graf graf--li graf-after--li"><a href="http://jmcauley.ucsd.edu/data/amazon/qa/" data-href="http://jmcauley.ucsd.edu/data/amazon/qa/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Amazon Q&amp;A dataset</a> — This data is huge and much of it is unnecessary, but you can take the yes or no questions from the data.</li><li name="3142" id="3142" class="graf graf--li graf-after--li"><a href="https://github.com/ashishrp96/Automated-Reply-Suggestion/blob/main/Custom_Dataset.csv" data-href="https://github.com/ashishrp96/Automated-Reply-Suggestion/blob/main/Custom_Dataset.csv" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">Custom dataset</a> — Created by us with 200 records to include more business use cases.</li></ol><p name="f94c" id="f94c" class="graf graf--p graf-after--li">Each dataset was sourced was cleaned and feature engineered to create data with only two columns “Input” for incoming messages and “reply” for their reply messages. We limited the length of input messages to 50 words or less. Similarly, for replies, we limited them to 10 words or less.</p><p name="c286" id="c286" class="graf graf--p graf-after--p">We clean the data from all these sources for punctuation, so we can utilize our text. we removed URLs, Emojis, hashtags, one-letter messages, messages in non-English languages, null rows, duplicates, and so on. We also change the higher case text to lower case for uniform input.</p><figure name="f889" id="f889" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*MxCMn3HsxWMf2BjSwS08Hg.png" data-width="530" data-height="405" src="https://cdn-images-1.medium.com/max/800/1*MxCMn3HsxWMf2BjSwS08Hg.png"><figcaption class="imageCaption">A snippet of our custom dataset with Input and reply columns</figcaption></figure><h3 name="bf42" id="bf42" class="graf graf--h3 graf-after--figure">Approach</h3><p name="5440" id="5440" class="graf graf--p graf-after--h3">Our Approach is divided into 4 parts:</p><ol class="postList"><li name="69a8" id="69a8" class="graf graf--li graf-after--p">Vectorization of the replies using <a href="https://github.com/pdrm83/sent2vec" data-href="https://github.com/pdrm83/sent2vec" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">sent2vec</a> — pre-trained <a href="https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection." data-href="https://www.techtarget.com/searchenterpriseai/definition/BERT-language-model#:~:text=BERT%2C%20which%20stands%20for%20Bidirectional,calculated%20based%20upon%20their%20connection." class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">BERT</a> model available in sent2vec package.</li><li name="18f8" id="18f8" class="graf graf--li graf-after--li">Clustering of the vectorized replies with Density-Based Spatial Clustering Application with Noise (DBSCAN) Algorithm — This produces clusters of replies that are semantically similar with labels.</li><li name="2c39" id="2c39" class="graf graf--li graf-after--li">Tokenize the input messages for the LSTM model</li><li name="b771" id="b771" class="graf graf--li graf-after--li">LSTM model takes the input messages tokens and predicts the top three reply cluster labels. One reply message from each reply cluster can be taken as our predicted reply — this way we generate reply that of different intents.</li></ol><figure name="5a73" id="5a73" class="graf graf--figure graf-after--li"><img class="graf-image" data-image-id="1*gFamR-rzpkOIWPa7akqKPg.png" data-width="924" data-height="445" src="https://cdn-images-1.medium.com/max/800/1*gFamR-rzpkOIWPa7akqKPg.png"><figcaption class="imageCaption">Approach</figcaption></figure><p name="6a25" id="6a25" class="graf graf--p graf-after--figure">So, first, we start with the vectorization of the replies. we used the Sent2vec vectorization technique, which vectorizes the sentences using the pre-trained BERT model. If all the sentences are represented as points in an N-dimensional space, similar sentences will be closer. We can use cosine similarity as a metric to measure their closeness.</p><figure name="eeeb" id="eeeb" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/ashishrp96/75b4e711eef9d9c5d4e2a2020be44d39#file-sent2vec-vectorization-of-data-py.js"></script></figure><p name="9b93" id="9b93" class="graf graf--p graf-after--figure">Now, we take vectorized reply messages and cluster them using the DBSCAN algorithm. This density-based algorithm was chosen for multiple reasons:</p><ol class="postList"><li name="e6dc" id="e6dc" class="graf graf--li graf-after--p">We do not need to provide the number of final clusters, as we don’t how many clusters of replies will form. The algorithm will find the number of clusters, which will be affected by the hyperparameters provided.</li><li name="9163" id="9163" class="graf graf--li graf-after--li">The algorithm uses cosine similarity to cluster the data points.</li><li name="7fed" id="7fed" class="graf graf--li graf-after--li">It identifies noise points — Noise points are data points that didn’t belong in any cluster. In this context, noise points could be random or very context-specific pieces of conversation.</li></ol><p name="1cac" id="1cac" class="graf graf--p graf-after--li">DBSCAN has two hyperparameters:</p><p name="800e" id="800e" class="graf graf--p graf-after--p">Epsilon — Two points are considered neighbors/or part of the same cluster if the distance between the two points is below the threshold epsilon.</p><p name="1943" id="1943" class="graf graf--p graf-after--p">Minimum number of samples per cluster — The minimum number of neighbors a given point should have in order to be classified as a core point.</p><p name="2560" id="2560" class="graf graf--p graf-after--p">For smaller epsilon values, clusters that form are tighter which means sentences that are very much closer semantically get into a cluster. As you keep increasing the epsilon value, clusters become looser. This means little less semantically similar sentences will also enter into the cluster.</p><p name="8e9a" id="8e9a" class="graf graf--p graf-after--p">For example, for small epsilon values, a cluster with “I’m doing good” may take sentences that are very close or the same like “I’m doing good, Thanks” enter the cluster. But as you increase epsilon values, less similar sentences which means almost similar like “I’m going great” will enter that cluster. We need reply clusters that consist of different replies which mean the same, so we need to find appropriate epsilon values through trials and check for average cosine similarity for clusters formed with each epsilon.</p><figure name="d207" id="d207" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/ashishrp96/46a1ea9214cb45ef9ad7c9549f1aeb60#file-clustering-using-dbsacn-py.js"></script></figure><figure name="4011" id="4011" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*lX4XqF77Cwdfnpw3znoSHQ.png" data-width="346" data-height="218" src="https://cdn-images-1.medium.com/max/800/1*lX4XqF77Cwdfnpw3znoSHQ.png"><figcaption class="imageCaption">An example reply cluster — tightly clustered</figcaption></figure><p name="11ae" id="11ae" class="graf graf--p graf-after--figure">After Clustering the replies and creating reply clusters, we come back to incoming messages. We now tokenize the incoming messages.</p><figure name="1f41" id="1f41" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/ashishrp96/a04f6daf06381947bd815e1bb9503ef2#file-tokenization-of-incoming-messages-py.js"></script></figure><p name="f122" id="f122" class="graf graf--p graf-after--figure">We then created a model with embedding layer and Encoder part of LSTM model. Generally, in sequence to sequence LSTM models, the encoder provides updated hidden state and updated cell state to the decoder. Since we are predicting or doing multi-label classification of N number of clusters formed. We consider the cell state tensor which contains the context/information all across the sentence can be used as our base for output, adding a fully connected layer to it will turn it into (Batch Size x Number of Labels) tensor and we used it to measure cross-entropy loss with output labels. We can train an LSTM model to become our classifier and the output will be provided as a tensor that holds probabilities of each label of reply cluster for each incoming message. From the probabilities, we select the top three probable labels as our predicted reply clusters.</p><figure name="bf4f" id="bf4f" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*OytRhoGhOKx2k3WhORqS_w.jpeg" data-width="1683" data-height="1152" src="https://cdn-images-1.medium.com/max/800/1*OytRhoGhOKx2k3WhORqS_w.jpeg"><figcaption class="imageCaption">Cell state updating in the LSTM cell. Source: Guillaume Chevalier, via <a href="https://commons.wikimedia.org/wiki/File:LSTM_Cell.svg" data-href="https://commons.wikimedia.org/wiki/File:LSTM_Cell.svg" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">Wikipedia</a> (CC BY-SA 4.0)</figcaption></figure><figure name="ee9b" id="ee9b" class="graf graf--figure graf--iframe graf-after--figure"><script src="https://gist.github.com/ashishrp96/0d229dba8dd8e84fdaf2dadc12e718d0#file-encoder-lstm-multi-label-classifier-py.js"></script></figure><h3 name="a7ac" id="a7ac" class="graf graf--h3 graf-after--figure">Implementation</h3><p name="002b" id="002b" class="graf graf--p graf-after--h3">I first started testing the approach on the cleaned Twitter dataset. we divide the data into a training-validation-test ratio of 70–20–10. The clustering model showed that</p><p name="1204" id="1204" class="graf graf--p graf-after--p">Number of clusters: 2470</p><p name="6a45" id="6a45" class="graf graf--p graf-after--p">Number of noise points: 222650</p><p name="f619" id="f619" class="graf graf--p graf-after--p">Total records: 268992</p><p name="5b60" id="5b60" class="graf graf--p graf-after--p">Epsilon = 0.0012<br>Minimum number of samples per cluster = 2</p><p name="3676" id="3676" class="graf graf--p graf-after--p">There is an overwhelming number of noise points in the data. I clustered tightly using a smaller epsilon value to detect a number of noise points. More than 80% of data points aren’t clustered. These noise data points if included down the line in the LSTM model will induce bias. LSTM model gave a false validation accuracy of 81% when noise points are included. Validation accuracy is the metric to measure LSTM model performance about how are accurately the model is predicting the reply cluster label of the ground truth reply on the validation dataset.</p><p name="7729" id="7729" class="graf graf--p graf-after--p">As DBSCAN algorithm puts all noise points into a noise cluster labeled “-1”. The LSTM becomes biased towards this “-1” cluster and hence this false accuracy. So, we need to remove these noise data points from the data. Hence, all the datasets went through the DBSCAN algorithm and their detected noise points were removed.</p><figure name="3dd7" id="3dd7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*NyuhSDslZGpe1qmDIhSmCg.png" data-width="471" data-height="237" src="https://cdn-images-1.medium.com/max/800/1*NyuhSDslZGpe1qmDIhSmCg.png"><figcaption class="imageCaption">Noise points examples</figcaption></figure><p name="31e9" id="31e9" class="graf graf--p graf-after--figure">We combine all the individual cleaned datasets into one new dataset. Once again, we divide the data into a training-validation-test ratio of 70–20–10. Now we need to fine-tune the hyperparameters in clustering.</p><figure name="9c50" id="9c50" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5dy4gHuqTEVVU4xydKGKuA.jpeg" data-width="468" data-height="270" src="https://cdn-images-1.medium.com/max/800/1*5dy4gHuqTEVVU4xydKGKuA.jpeg"><figcaption class="imageCaption">Number of cluster and number of noise points formed varying with epsilon</figcaption></figure><figure name="58b4" id="58b4" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*0ZDasPo_7M7ItYLWQgU2Hw.jpeg" data-width="467" data-height="267" src="https://cdn-images-1.medium.com/max/800/1*0ZDasPo_7M7ItYLWQgU2Hw.jpeg"><figcaption class="imageCaption">Number of cluster and average cosine similarity of clusters formed varying with epsilon</figcaption></figure><p name="8a55" id="8a55" class="graf graf--p graf-after--figure">We used cosine similarity here again as a metric to check the sentence similarity in reply clusters formed. First, you vectorize the sentences in each reply cluster using Tf-Idf vectorization or you can use the already available sent2vec vectors and find average or median cosine similarity between all the combinations of sentences. Then you need to average the cosine similarity across all the clusters formed to get the final average cosine similarity value.</p><p name="378d" id="378d" class="graf graf--p graf-after--p">You can the number of clusters formed and the number of noise points found corresponding based on the value of epsilon used in the DBSCAN algorithm. The number of clusters and noise points goes down by increasing the epsilon value. the cosine similarity also goes down but increases again. This was an interesting development, we theorized that this was due to increasing epsilon more brought more similar sentences across all the clusters, so the average value increased instead of going down as we expected.</p><p name="6a44" id="6a44" class="graf graf--p graf-after--p">So, we finally chose the epsilon value 0.44 as our final value for its generation of the reasonable number of clusters and its average cosine similarity.</p><p name="cb7c" id="cb7c" class="graf graf--p graf-after--p">Taking Epsilon = 0.0044<br>Minimum samples in cluster = 4<br>Metric = cosine similarity<br>Estimated number of clusters: 1016<br>Estimated number of noise points: 1696</p><p name="ec1a" id="ec1a" class="graf graf--p graf-after--p">we got 1016 labeled reply clusters and we have to make note that having a lower number of classes would make it an easier job for the LSTM model to classify. So lowering our number of clusters will also help the LSTM model.</p><p name="a3ea" id="a3ea" class="graf graf--p graf-after--p">Now coming to LSTM model hyperparameter tuning, we used the <a href="https://wandb.ai/site/sweeps" data-href="https://wandb.ai/site/sweeps" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Weights and Biases Sweeps tool</a>. we used random search with various combinations of the hyperparameters like the number of epochs, optimizer, hidden size, embedding dimensions, and learning rate. Embedding size is a parameter for the embedding layer of the model. Hidden size is a parameter for the fully connected layer. We tried between two optimizers “adam” and “SGD”.</p><figure name="10a3" id="10a3" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/ashishrp96/a04c5d4941101f02fe0a6fc274a8eddc#file-wandb-parameter-dictionary-py.js"></script></figure><p name="1f45" id="1f45" class="graf graf--p graf-after--figure">We ran sweeps with a train function that checks validation accuracy for the model.</p><figure name="458d" id="458d" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/ashishrp96/7adf0169e299fd024a26779e60bc4c75#file-wandb-sweeps-py.js"></script></figure><p name="1152" id="1152" class="graf graf--p graf-after--figure">We got visualized results from sweeps like this.</p><figure name="373e" id="373e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*1fHsQM4D1VzR_R9qu6FMEw.jpeg" data-width="1073" data-height="460" src="https://cdn-images-1.medium.com/max/800/1*1fHsQM4D1VzR_R9qu6FMEw.jpeg"><figcaption class="imageCaption">Different variations of hyper-parameters for the model giving different accuracy values</figcaption></figure><p name="b126" id="b126" class="graf graf--p graf-after--figure">We found the best values for hyperparameters:</p><p name="5257" id="5257" class="graf graf--p graf-after--p">Embedding dimension = 128<br>Hidden size = 128<br>Learning rate = 0.001<br>Optimizer = Adam<br>Epochs = 15</p><p name="70e2" id="70e2" class="graf graf--p graf-after--p">The final model is trained on these values and tested on the validation dataset to check their performance. We have three metrics to measure the model performance, some of them were introduced before like validation accuracy.</p><p name="ebbb" id="ebbb" class="graf graf--p graf-after--p">1. Cross-Entropy Loss — Cross-entropy is a measure of the difference between two probability distributions for a given random variable or set of events. It is measured between the output from the LSTM model probabilities and Ground truths (Actual Labels in the validation dataset).</p><p name="ab8e" id="ab8e" class="graf graf--p graf-after--p">2. Validation Accuracy — If one of the three predictions is correct as to what is there as ground truth labels.</p><p name="3122" id="3122" class="graf graf--p graf-after--p">3. Cosine Similarity — Like previously discussed, it is a metric to measure sentence similarity from 0 to 1. We take the average cosine similarity of our predictions with the ground truth.</p><h3 name="ec57" id="ec57" class="graf graf--h3 graf-after--p">Final results</h3><p name="efdf" id="efdf" class="graf graf--p graf-after--h3">Cross-entropy Loss:1.46</p><p name="a612" id="a612" class="graf graf--p graf-after--p">Validation Accuracy: 74.82%</p><p name="33f8" id="33f8" class="graf graf--p graf-after--p">Average Cosine Similarity: 0.52</p><p name="9679" id="9679" class="graf graf--p graf-after--p">Cosine similarity is not our best metric because we are not supposed to predict exact replies but replies that are general and apt, so cosine similarity which measures sentence similarity becomes less relevant here.</p><p name="83b6" id="83b6" class="graf graf--p graf-after--p">Our main metric is validation accuracy which is at 74.82%, which means that the model is predicting three replies out of which at least one of them belongs to the same reply cluster as of the ground truth reply.</p><p name="47e8" id="47e8" class="graf graf--p graf-after--p">Here are some of the sample results from the model.</p><figure name="d588" id="d588" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*qA5Cn_KbvPcZ8H4JP-SUUQ.jpeg" data-width="527" data-height="134" src="https://cdn-images-1.medium.com/max/800/1*qA5Cn_KbvPcZ8H4JP-SUUQ.jpeg"></figure><figure name="7f47" id="7f47" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*zmQs2rKpPqw5shjkKOax_w.jpeg" data-width="443" data-height="106" src="https://cdn-images-1.medium.com/max/800/1*zmQs2rKpPqw5shjkKOax_w.jpeg"></figure><figure name="ee0e" id="ee0e" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*ICbaf8ho6BWKsmlAOf_bNA.jpeg" data-width="431" data-height="123" src="https://cdn-images-1.medium.com/max/800/1*ICbaf8ho6BWKsmlAOf_bNA.jpeg"></figure><p name="b02a" id="b02a" class="graf graf--p graf-after--figure">I’ve also implemented an end-user UI web app using <a href="https://gradio.app/" data-href="https://gradio.app/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Gradio</a>.</p><figure name="cd07" id="cd07" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*YYg7UL9mxAJEgF_yolV_XQ.jpeg" data-width="780" data-height="198" src="https://cdn-images-1.medium.com/max/800/1*YYg7UL9mxAJEgF_yolV_XQ.jpeg"></figure><h3 name="25fa" id="25fa" class="graf graf--h3 graf-after--figure">Additional work</h3><p name="8993" id="8993" class="graf graf--p graf-after--h3">The reply clusters can be further cleaned by checking their relevancy for the business use cases. we can eliminate reply clusters that still consist of inappropriate replies. This can be done manually which is what we did because of time constraints, but it can be automatized by building another model that would identify inappropriate reply clusters.</p><p name="8bf6" id="8bf6" class="graf graf--p graf-after--p">We assigned a default reply for each reply cluster by finding the centroid of each cluster. DBSCAN doesn’t provide centroids as it’s a density-based clustering. But we can find centroids for each cluster mathematically by using the vectors for each reply message created by the Sent2vec technique. By doing this, we can provide consistent replies for incoming messages.</p><h3 name="5b41" id="5b41" class="graf graf--h3 graf-after--p">Additional Modeling</h3><p name="e2d2" id="e2d2" class="graf graf--p graf-after--h3">We used Multilayer perception in place of LSTM by vectorizing input messages with Sent2vec but it gave validation accuracy of 71%, so we chose better performing LSTM instead.</p><p name="e3c5" id="e3c5" class="graf graf--p graf-after--p">We also tried a model called the Correlation matrix model which is similar to naive Bayes. We vectorized incoming messages with Sent2vec as we did for the MLP model also, then we clustered them using DBSCAN. Now we created a correlation matrix between input clusters and reply clusters by checking how many times each reply label occurred in input clusters. So, we’d get a Correlation matrix which when normalized across the axis becomes a probability matrix like in naive Bayes. This matrix can be used to predict the top three clusters for each input cluster. To predict a reply for a new incoming message, we vectorize the message, then cluster it with the input clustering model and provide the top three corresponding reply clusters. This method though interesting in its idea failed with 0.72% accuracy.</p><p name="5945" id="5945" class="graf graf--p graf-after--p">We also tried the Support Vector Machine model along with Sent2vec vectorization of input messages again in place of LSTM. Though effective, SVM predicts only one reply cluster. This was discarded quickly as we need more than one reply suggestion.</p><h3 name="ba9b" id="ba9b" class="graf graf--h3 graf-after--p">Future Work</h3><p name="6898" id="6898" class="graf graf--p graf-after--h3">For future work my recommendations are:</p><p name="4694" id="4694" class="graf graf--p graf-after--p">1. Data Collection — data needs to be more robust and much varied and relevant to the model’s use case.</p><p name="0e69" id="0e69" class="graf graf--p graf-after--p">2. Model Building — Using Attention-based Transformer model instead of LSTM because this model takes more information from sentences like the local context of words and can provide better results.</p><p name="5539" id="5539" class="graf graf--p graf-after--p">3. Trigger models to check whether the incoming models need suggestions.</p><p name="a39c" id="a39c" class="graf graf--p graf-after--p">4. Model to weed out irrelevant and inappropriate reply clusters.</p><p name="c19e" id="c19e" class="graf graf--p graf-after--p">5. Automatically assign a default reply to each cluster by finding its centroid or something equivalent to the centroid.</p><p name="cdc9" id="cdc9" class="graf graf--p graf-after--p">6. Using Sent2vec vectorizer instead of embedding layer for LSTM model.</p><h3 name="5ba7" id="5ba7" class="graf graf--h3 graf-after--p">Project Source Repository</h3><p name="c323" id="c323" class="graf graf--p graf-after--h3">This project entire code and datasets are available here:</p><div name="fc0b" id="fc0b" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/ashishrp96/Automated-Reply-Suggestion" data-href="https://github.com/ashishrp96/Automated-Reply-Suggestion" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/ashishrp96/Automated-Reply-Suggestion"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - ashishrp96/Automated-Reply-Suggestion: Deep Learning architecture for Automated reply…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Deep Learning architecture for Automated reply suggestion. Uses machine learning models, deep learning models and NLP…</em>github.com</a><a href="https://github.com/ashishrp96/Automated-Reply-Suggestion" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="466870eea29fd698998573026838c209" data-thumbnail-img-id="0*IW27BcHFj6cK30yj" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*IW27BcHFj6cK30yj);"></a></div><h3 name="155f" id="155f" class="graf graf--h3 graf-after--mixtapeEmbed">Collaborators and Mentors</h3><p name="4f36" id="4f36" class="graf graf--p graf-after--h3">My main collaborator with me on this project is <a href="https://www.linkedin.com/in/ushasee-das-7404a5114/" data-href="https://www.linkedin.com/in/ushasee-das-7404a5114/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ushasee Das</a>.</p><p name="0a5d" id="0a5d" class="graf graf--p graf-after--p">Our Rutgers MBS mentors on this project are:</p><p name="e119" id="e119" class="graf graf--p graf-after--p"><a href="https://www.linkedin.com/in/bryan-bischof/" data-href="https://www.linkedin.com/in/bryan-bischof/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dr. Bryan Bischof</a></p><p name="6d6c" id="6d6c" class="graf graf--p graf-after--p"><a href="https://mbs.rutgers.edu/staff/dr-christie-nelson" data-href="https://mbs.rutgers.edu/staff/dr-christie-nelson" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Dr. Christie Nelson</a></p><p name="3d89" id="3d89" class="graf graf--p graf-after--p">Our Mentors from Infragistics Team are:</p><p name="4882" id="4882" class="graf graf--p graf-after--p"><a href="https://www.linkedin.com/in/tobiaskomischke/" data-href="https://www.linkedin.com/in/tobiaskomischke/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Tobias Komischke</a></p><p name="9c2c" id="9c2c" class="graf graf--p graf-after--p graf--trailing"><a href="https://www.linkedin.com/in/vaibhavmalik10/" data-href="https://www.linkedin.com/in/vaibhavmalik10/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Vaibhav Malik</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@ap1822" class="p-author h-card">Ashish Podduturi</a> on <a href="https://medium.com/p/f297cb26c003"><time class="dt-published" datetime="2022-01-08T05:51:20.414Z">January 8, 2022</time></a>.</p><p><a href="https://medium.com/@ap1822/automated-reply-suggestion-using-natural-language-processing-techniques-f297cb26c003" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on January 15, 2022.</p></footer></article></body></html>
